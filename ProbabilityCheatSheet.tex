\documentclass{article}
\usepackage[letterpaper, portrait, margin=0.5in]{geometry}
\usepackage{capt-of}
\usepackage{amsmath}
\begin{document}
	\section{Counting}
	\begin{tabular}{ |c|c| } 
		\hline
		Ordered Sampling with replacement & $n^k$ \\
		\hline
		Ordered Sampling without replacement & ${^nP_k} = \frac{n!}{(n-k)!}$\\
		\hline
		Unordered Sampling with replacement & $\binom{n}{k} = \frac{n!}{k!(n-k)!}$\\
		\hline
		Unordered Sampling with replacement & $\binom{n+k-1}{k} = \binom{n+k-1}{n-1}$\\
		\hline
		The Binomial Theorem & $(x+y)^n = \sum{\binom{n}{k}x^ky^{n-k}}$\\
		\hline
		The Multinomial Theorem & $(x_1 + x_2 + ... + x_r)^n = \sum\limits_{\substack{(n_1, n_2, ... n_r): \\ n_1 + ... + n_r = n}} \frac{n!}{n_1!n_2!...n_r!}x_1^{n_1}x_2^{n_2}...x_r^{n_r}$\\
		\hline
	\end{tabular}
	\newline
	There are $\binom{n+k-1}{k-1}$ distinct nonnegative integer-valued vectors $<x_1,..., x_r>$ satisfying $x_1 + ... + x_k = n$
	\section{Probability}
	Probability Rules
	\newline
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		 $\left(\bigcup_{i=1}^{n} E_i\right)^c = \bigcap_{i=1}^{n} E_i$ &  $\left(\bigcap_{i=1}^{n} E_i\right)^c = \bigcup_{i=1}^{n} E_i$ & $P(E^c) = 1 - P(E)$ & $P(E \cup F) = P(E) + P(F) - P(EF)$ & $P(E \vert F) = \frac{P(EF)}{P(F)}$ \\
		\hline
	\end{tabular}
	\begin{tabular}{|c|}
		$P(E) = P(EF) + P(EF^c) = P(F)P(E \vert F) + P(F^c)P(E \vert F^c) = P(F)P(E \vert F) + (1 - P(F))P(E \vert F^c)$ \\
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|}
		\hline
		Law of Total Probability & $P(E) = \sum_{i=1}^n P(E \vert F_i)P(F_i)$ \\
		\hline
		Bayes' Theorem & Suppose that $F_1, F_2, ... F_n$ are mutually exclusive events such that $\bigcup_{i=1}^n F_i = S$ (Sample Space), \\ & then $P(F_j \vert E) = \frac{P(EF_j)}{P(E)} = \frac{P(E \vert F_j)P(F_j)}{\sum_{i=1}^n P(E \vert F_i)P(F_i)}$\\
		\hline
		Independent Events & Two events are independent if $P(EF) = P(E)P(F)$ \\
		\hline
	\end{tabular}
	
	\section{Discrete Random Variable}
	\begin{tabular}{|c|c|}
		\hline
		Probability Mass Function (PMF) & $p_X(a) = P\{X = a\}$\\
		\hline
		Cumulative Distribution Function (CDF) of X & $F_X(a) = P\left\{X <= a\right\} = \sum_{x<=a} p_X(a)$\\
		\hline
		Expected Value & $E[X] = \sum_{x} xp_X(x)$ \\
		\hline
		For any function of $g$ & $E[g(X)] = \sum_{x} g(x)p_X(x)$\\
		\hline
		$Var(X) = E[X^2] - (E[X])^2$ & $SD(X) = \sqrt{Var(X)}$ \\ & $Var(aX + b) = a^2Var(X)$ \\
		\hline
	\end{tabular}
	\newline
	Distributions
	\begin{tabular}{|c|c|c|c|}
		\hline
		Name & PMF & Mean & Variance\\
		\hline
		$Bernoulli(p)$ & $P\{X = 1\} = p$, $P\{X = 0\} = 1 - p$ & $p$ & $p((1-p)$\\
		\hline
		$Binomial(n, p)$ & $P\{X = k\} = \binom{n}{k}p^k(1-p)^{n-k}, k = 0,..., n$ & $np$ & $np((1-p)$\\ 
		\hline
		$Geometric(p)$ & $P\{X = n\} = p(1-p)^{n-1}, n= 0, 1, 2, ...$ & $\frac{1}{p}$ & $\frac{1-p}{p^2}$\\
		\hline
		$Poisson(\lambda)$ & $P\{X = n\} = e^{\lambda}\frac{\lambda^n}{n!}, n = 0, 1,...$ & $\lambda$ & $\lambda$ \\
		\hline
		$negbinomial(r, p)$ & $P\{X = r\} = \binom{n - 1}{r - 1}p^r(1- p)^{n-r}, n = r, r + 1, ... $ & $\frac{r}{p}$ & $r\frac{(1 - p)}{p^2}$\\
		\hline
		$hypergeometric$ & $P\{X = i\} = \frac{\binom{m}{i}\binom{N - m}{n - i}}{\binom{N}{n}}, i = 0, 1, 2,..., min(n, m)$ & $\frac{nm}{N}$ & $\frac{N - n}{N - 1}np(1 -p) where p = m/N$\\
		\hline
	\end{tabular}
	\section{Continuous Random Variables}
	 \begin{tabular}{|c|c|}
		\hline
		Probability Mass Function (PMF) & $f_X(a) = \frac{1}{\epsilon}P\{a - \frac{\epsilon}{2} < = X <= a + \frac{\epsilon}{2}\}$\\
		\hline
		Cumulative Distribution Function (CDF) of X & $F_X(a) = P\{X <= a\} = \int_{-\infty}^{a} p_X(a)dx$\\
		\hline
		$P\{X \in B\} = \int{B}f_X(x)dx$ & $f_X(x) = \frac{d}{dx}F_X(x)$\\
		\hline
		Expected Value & $E[X] = \int_{-\infty}^{\infty} xf_X(x)$ \\
		\hline
		For any function of $g$ & $E[g(X)] = \int_{-\infty}^{\infty} g(x)f_X(x)$\\
		\hline
		$Var(X) = E[X^2] - (E[X])^2$ & $SD(X) = \sqrt{Var(X)}$ \\ & $Var(aX + b) = a^2Var(X)$ \\
		\hline
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Name & PMF & CDF & Mean & Variance\\
		\hline
		$uniform(a, b)$ 
			& $f(x) = \begin{cases} \frac{1}{b - a} & \text{if } a < x < b \\  0 & \text{othewrise}\end{cases}$  
			& $F(x) = \begin{cases} 0 & x <= a \\ \frac{x -a}{b -a} & \text{if } a < x < b \\ 1 & x >= a \end{cases} $
			& $\frac{a + b}{2}$ 
			& $\frac{(b - a)^2}{12}$\\
		\hline
			$Exponential(\lambda)$ 
			& $f(x) = \begin{cases} \lambda e^{-\lambda x} & \text{if } x >= 0\\  0 & \text{othewrise}\end{cases}$  
			& $F(x) = 1 - e^{-\lambda x} \text{   if } a >= 0$
			& $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ \\
		\hline
			$Normal(\mu, \sigma^2)$ 
			& $f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma^2}$
			& $\phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-u^2/2}du \text{   *Use normal table}$
			& $\mu$
			&$\sigma^2$\\
		\hline
	\end{tabular}
	\newline
	For any normal random variable $X$ with parameters $(\mu, \sigma^2)$, $Z = \frac{X - \mu}{\sigma}$ is the standardized normal random variable.
	\newline
	\underline{DeMoivere-Laplace Limit Theorem:} If $S_n$ denotes the number of successes that occur when n independent trials, each resulting in asuccess with probability p, are performed then, for any a < b, $P\{a <= \frac{S_n -np}{\sqrt{np(1 -p)}} <= b\} -> \phi(b) - \phi(a)$
	\newline
	\underline{Memoryless RV: } We say that a nonnegative random variable $X$ is $memoryless$ if $P\{X >s + t \vert X > t\} = P\{X > s\}$
	\newline
	\underline{Distribution of a Function of a RV: } $f_Y(y) = f_X[g^{-1}(y)] \vert{\frac{d}{dy}g^{-1}(y)}\vert \text{  if } y = g(x) \text{ for some } x$
	\section{Jointly Distributed Random Variables}
	\underline{Note: } Replace summation with integration for continuous
	\newline
	\begin{tabular}{|c|c|c|c|}
		\hline
		Joint CDF & $f_{XY}(a,b) = P\{X = a, Y= b\}$ & Joint CDF & $F_{XY}(a,b) = P\{X <= a, Y <= b\}$\\
		\hline
		Marginal PMF of X & $p_X(a) = \sum_{b}p_{XY}(a,b)$ & Marginal PMF of Y & $p_Y(b) = \sum_{a}p_{XY}(a,b)$ \\
		\hline
		Marginal CDF of X &  $F_{XY}(a) = F_{XY}(a,\infty)$ & Marginal CDF of Y & $F_XY(b) = F_{XY}(\infty, b)$ \\
		\hline
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|}
		Conditional PMF & $p_{X \vert Y}(x \vert y) = P\{X = x \vert Y = y\} = \frac{P\{X = x, Y = y\}}{P\{Y = y\}} = \frac{p_{XY}(x,y)}{p_Y(y)}$ \\
		\hline
		Conditional CDF & $F_{X \vert Y}(x \vert y) = P\{X <= x \vert Y <= y\} = \sum_{a <= x} p_{X \vert Y}(a \vert y) $\\
		\hline
		Given Function $g(X,Y)$ & $\sum_{y}\sum_{x}g(x,y)p_{XY}(x,y)$ \\
		\hline
	\end{tabular}
	\newline
	Conditional Jointly Distributed RV
	\newline
	\begin{tabular}{|c|c|c|}
		\hline
		$F_{X \vert A}(x) = \frac{F_X(x) - F_X(a)}{F_X(b) - F_X(a)} \text{   if} a <= x < b$ & $f_{X \vert A}(x) = \frac{f_X(x)}{P(A)}$ & $f_{X \vert Y}(x, y) = \frac{f_{XY}(x, y)}{f_Y(y)}$\\
		\hline
		$E[X \vert A] = \int_{-\infty}^{\infty}xf_{X \vert A}(x)dx$ & $E[g(X) \vert A] = \int_{-\infty}^{\infty}g(x)f_{X \vert A}(x)dx$  		& $$\\
		\hline
	\end{tabular}
	\newline
	\underline{Independent Random Variables:} Two random variables $X$ and $Y$ are said to be independent if for every $a$ and $b$ we have $f_{XY}(a, b) = f_{X}(a)f_{Y}(b)$. 
	\newline
	\underline{Summation of Independent Random Variables}
	\begin{tabular}{|c|c|}
		\hline
		CMF & $F_{X+Y}(a) = P\{X + Y < = a\}  = \int_{-\infty}^{\infty}F_X(a - y)f_Y(y)dy$\\
		\hline
		PDF & $f_{X+Y}(a) = \int_{-\infty}^{\infty}f_X(a - y)f_Y(y)dy$\\
		\hline
	\end{tabular}
	\newline
	\underline{Sum of Normal RV:} Mean and Variance are the summation of all the normal RV's mean and variance.
	\newline
	\underline{Jacobian Transformation} (Joint Probability Distribution functions of RV) 
	\newline
	\begin{tabular}{|c|c|c|}
		\hline
		$f_{Y_1, Y_2}(y_1, y_2) = \frac{f_{X_1, X_2}(x_1, x_2)}{\vert J(x_1, x_2) \vert}$
		& $J(x_1, x_2) = \begin{vmatrix} \frac{\partial g_1}{\partial x_1} & \frac{\partial g_1}{\partial x_2} \\ \frac{\partial g_2}{\partial x_1} & \frac{\partial g_2}{\partial x_2}\end{vmatrix} = \frac{\partial g_1}{\partial x_1}\frac{\partial g_2}{\partial x_2} - \frac{\partial g_1}{\partial x_2}\frac{\partial g_2}{\partial x_1}  \neq 0$ 
		& $y_1 = g_1(x_1, x_2)$ and $y_2 = g_2(x_1, x_2)$\\
		\hline
	\end{tabular}
	
	\section{Properties of Expectation}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Joint PMF & $E[g(X, Y)] = \sum_{y}\sum_{x}g(x, y)p(x, y)$ &
		Joint PDF &  $E[g(X, Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x, y)p(x, y)dxdyy$\\
		\hline
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|}
		\hline
		$E[aX+ b] = aE[X] + b$ & $E[g(X)h(Y)] = E[g(X)]E[h(Y)]$ if $X$ and $Y$ are independent\\
		\hline
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|}
		\hline
		$Cov(X, Y) = E[XY] - E[X]E[Y]$ & $Cov(X, Y)  = Cov(Y, X)$\\
		\hline
		$Cov(X, X) = Cov(X, X)$ & $Cov(aX,Y) = aCov(X, Y)$\\ 
		\hline
		$Cov(\sum_{i=1}^nX_i, \sum_{j=1}^mY_j) = \sum_{i=1}^n\sum_{j=1}^mCov(X_i,Y_j)$  &
		$Var(\sum_{i = 1}^nX_i) = \sum_{i = 1}^nVar(X_i) + \sum_{i = 1}^n\sum_{j \neq i}Cov(X_i, X_j)$\\
		\hline
	\end{tabular}
	\newline
	$Var(\sum_{i = 1}^nX_i) = \sum_{i = 1}^nVar(X_i)$ if $X1,... , Xn$ are pairwise independent 
	\newline
	\underline{Correlation: } $\rho(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}} $ where $-1 <= \rho(X,Y) <= 1$
	\newline
	\begin{tabular}{|c|c|c|}
		\hline
		Conditional Expectation & $E[X \vert Y = y] = \sum_{x}xp_{X \vert Y}(x, y)$ & $E[g(X) \vert Y = y] = \sum_{x}g(x)p_{X \vert Y}(x, y)$\\
		\hline
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|c|c|}
		Conditioning Property & $E[X] = E[E[X \vert Y]]$ &
		Expectation of X & $\sum_{y}E[X \vert Y = y]P\{Y = y\}$\\
		\hline
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|}
		\hline
		Conditional Variance & $Var(X \vert Y) = E[X^2 \vert Y] - (E[X \vert Y])^2$\\
		\hline
		Unconditional Variance of X given Y & $E[Var(X \vert Y)] = E[X^2] - E[(E[X \vert Y])^2]$\\
		\hline
		$Var(E[X \vert Y]) = E[(E[X \vert Y])^2] - (E[X])^2$ & $Var(X) = E[Var(X \vert Y)] + Var(E[X \vert Y])$\\
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|c|}
		\hline
		General Form Predictor & $E[(Y - g(X))^2] >= E[(Y - E[Y \vert X])^2]$ & best case is $g(X) = E[Y \vert X]$\\
		\hline
		Linear Predictor & $E[(Y - (a + bX))^2] >= E[(Y - E[Y \vert X])^2]$ & $b = \rho\frac{\sigma_Y}{\sigma_X}$ and $a = E[Y] - bE[X]$\\
		\hline
	\end{tabular}
	\newline
	\begin{tabular}{|c|c|}
		\hline
		MGF of PDF & $M(t) = E[e^{tX}] = \sum_{x}e^{tx}p(x)$\\
		\hline
		MGF of PMF & $M(t) = E[e^{tX}] = \int_{-\infty}^{\infty}e^{tx}f(x)dx$\\
		\hline
		More than 2 RV MGF & $M(t_1, ..., t_n) = E[e^{t_1x_1 + ... + t_nX_n}]$\\
		\hline
		Individual MGF & $M_{X_i}(t) = M(0,... , 0, t, 0,..., 0)$\\
		\hline
		Joint MGF & $M_{XY}(t, t') = E[e^{tX+T' Y}] = \sum_{y}\sum{x}e^{tx+t' y}p_{XY}(x,y)$ \\
		\hline
		MGF of IRV & $M_{X+Y}(t) = M_X(t)M_Y(t)$\\
		\hline
	\end{tabular}
	\underline{Nth Moment: } $M^n(0) = E[X^n]$ $n >= 0$
	\newline
	Moment Generating Functions for Some RV
	\begin{tabular}{|c|c|}
		\hline
		Binomial & $M(t) = (pe^t + 1 - p)^n$ \\
		\hline
		Poisson & $M(t) = exp\{\lambda(e^t - 1)\} $\\
		\hline
		Exponential & $M(t) = \frac{\lambda}{\lambda - t }$ for $t < \lambda$\\
		\hline
		Standard Normal & $M(t) = e^{t^2/2}$ \\
		\hline
		Normal & $exp\{\frac{\sigma^2t^2}{2} + {\mu}t\}$\\
		\hline
	\end{tabular}
	\section{Limit Theorems}
	\underline{Markov's Inequality:} 
	\newline If $X$ is a random variable that takes only nonnegative values, then for any value $a > 0$, $P\{X >= a|\} <= \frac{E[X]}{a}$
	\newline
	\underline{Chebyshev's Inequality:}
	\newline If $X$ is a random variable with finite mean $\mu$ and variance $\sigma^2$, then for any value $k > 0$, $P\{\vert X - \mu\vert >= k\} <= \frac{\sigma^2}{k^2}$
	\newline
	\underline{The Weak Law of Large Numbers}
	\newline
	Let $X_1, X_2, ...$ be a sequence of iid Random variables, each having finite mean. Then, for any $\epsilon > 0$, $P\{\vert\bar{X} - \mu\vert >= \epsilon\} \to 0$ where $\bar{X} = \frac{X_1 + X_2 + ... + X_n}{n}$  as $n\to\infty$
	\newline
	\underline{The Central Limit Theorem}
	\newline
	Let $X_1, X_2,...$ be a sequence of iid random variables, each having mean and variance. Then, the distribution of $Z = \frac{X_1 + ... + X_n - n\mu}{\sigma\sqrt{n}}$ tends to the standard normal as $n\to\infty$. That is, for $-\infty < a < \infty$, $P\{Z <= a\} = \phi(a) \to N(0,1)$ as $n\to\infty$
	\underline{CLT for Independent Random Variables}
	\newline
	Let $X_1, X_2, ...$ be a sequence of independent random variables having respective means and variances $\mu = E[X_i]$, $\sigma_i^2 = Var(X_i)$. If (a) the $X_i$ are uniformly bounded; that is, if for some $M$, $P\{\vert X_i\vert < M\} = 1$ for all $i$, and (b) $\sum_{i = 0}^{\infty}\sigma_i^2 = \infty$, then $P\{\frac{\sum_{i = 1}^n(X_i -\mu_i)}{\sqrt{\sum_{i = 1}^n\sigma_i^2}}\} \to \phi(a)$ as $n \to \infty$.
	\section{Random Processes}
	\section{Random Signal Processing}
\end{document}